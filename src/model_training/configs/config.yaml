defaults:
  rng_seed: 0xa1221f97
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 32
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  weight_decay: 0.00
  warmup_steps: 600
  eval_steps: 200
  save_strategy: steps
  save_steps: 1000
  max_length: 512
  val_max_length:
  num_train_epochs: 3
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - webgpt
    - squad_v2
    - adversarial_qa
    - trivia_qa_nocontext
    - xsum
    - cnn_dailymail
    - multi_news
    - scitldr
    - soda:
        input_max_length: 1024
    - joke
    - gsm8k
    - dive_mt
    - wmt2019_zh-en
    - wmt2019_ru-en
    - wmt2019_de-en
    - ted_trans_nl-en
    - ted_trans_de-ja
    - wmt2019_de-en
    - samsum
    - soda_dialogue
  # instructional_datasets:
  #  - humaneval_mbpp_codegen_qa
  #  - humaneval_mbpp_testgen_qa
  #  - grade_school_math_instructions
  #  - recipes
  #  - ubuntu_dialogue_qa
  #  - cmu_wiki_qa
  #  - youtube_subs_howto100M
  #  - iapp_wiki_qa_squad
  #  - zhihu-kol
  datasets_extra: [] # For config options to add additional datasets, since yaml doesn't let us extend arrays
  cache_dir: .cache
  loss_fn: CrossEntropyLoss
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  poly_eps: 1.0
  fuse_gelu: true
  log_wandb: true
  samples_mixing: false # uses collator that mixes samples in the batch to create a single sample with possible multiple tasks within
  verbose: false
  output_dir: saved_model
  use_custom_sampler: false
  random_offset_probability: 0.8 # probability for random message offsets
  label_masking: true
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  use_system_prefix: false
  system_prefix:
    "You are Agni, LLM by Adobe. Answer as concisely as possible"
  per_digit_tokens: false
  is_reward_model: false
  deepspeed_config: configs/zero_config.json

webgpt_dataset_only:
  datasets:
    - webgpt

per_digit_tokens:
  per_digit_tokens: true

math:
  datasets_extra: # Will get merged with datasets
    - minimath

pretrain:
  num_train_epochs: 1
  weight_decay: 0.0
  use_custom_sampler: true
  sort_by_length: false
  datasets:
    - joke:
        val_split: 0.05
    - webgpt:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - alpaca:
        val_split: 0.025
        max_val_set: 250
    - code_alpaca:
        val_split: 0.05
        max_val_set: 250
    - vicuna:
        max_val_set: 250
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl
        max_count: 10000
        min_length: 250
        val_split: 0.05
        max_val_set: 250
    - minimath:
        val_split: 0.05
    - humaneval_mbpp_codegen_qa:
        val_split: 0.05
    - humaneval_mbpp_testgen_qa:
        val_split: 0.05
    - grade_school_math_instructions:
        val_split: 0.05
    - recipes:
        val_split: 0.05
    - cmu_wiki_qa:
        val_split: 0.05
    - oa_wiki_qa_bart_10000row:
        val_split: 0.05
        max_val_set: 250
    - prosocial_dialogue:
        fraction: 0.1
        max_val_set: 250
    - explain_prosocial:
        fraction: 0.05
        max_val_set: 250
    - soda:
        fraction: 0.2
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250

pretrainv1:
  num_train_epochs: 1
  weight_decay: 0.0
  use_custom_sampler: true
  sort_by_length: false
  datasets:
    - webgpt:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - alpaca:
        val_split: 0.025
        max_val_set: 250
    - code_alpaca:
        val_split: 0.05
        max_val_set: 250
    - vicuna:
        max_val_set: 250
    - minimath:
        val_split: 0.05
    - humaneval_mbpp_codegen_qa:
        val_split: 0.05
    - humaneval_mbpp_testgen_qa:
        val_split: 0.05
    - grade_school_math_instructions:
        val_split: 0.05
    - cmu_wiki_qa:
        val_split: 0.05
    - prosocial_dialogue:
        fraction: 0.1
        max_val_set: 250
    - explain_prosocial:
        fraction: 0.05
        max_val_set: 250
    - soda:
        fraction: 0.1
        max_val_set: 250
    - shp:
        val_split: 0.05
        max_val_set: 250
    - dolly15k:
        val_split: 0.05
        max_val_set: 250
    - poem_instructions:
        fraction: 0.1
        val_split: 0.05
        max_val_set: 250


Lamem:
  dtype: bf16
  log_dir: "logs"
  learning_rate: 2e-5
  model_name: 
  log_wandb: true
  weight_decay: 0.0
  max_length: 1500
  warmup_steps: 20
  gradient_checkpointing: true
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  eval_steps: 20
  save_steps: 10
  save_strategy: steps
  logging_steps: 1
  num_train_epochs: 2
  save_total_limit: 15
  use_flash_attention: false # manually patched
  residual_dropout: 0.25
  deepspeed_config: configs/zero3_config_sft.json
  use_custom_sampler: true
  sort_by_length: false
  
  time: random

  output_dir: 
  datasets:
    - lamem:
        val_split: 0.05
       
   